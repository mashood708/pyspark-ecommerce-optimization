# pyspark-ecommerce-optimization
pyspark-ecommerce-optimization PySpark project demonstrating large-scale e-commerce transaction data optimization techniques with repartition, coalesce, caching, broadcast joins, and Catalyst optimizer.

# PySpark E-Commerce Optimization Project ðŸš€

This project demonstrates advanced PySpark optimization techniques on a simulated large-scale e-commerce transactions dataset. Techniques covered:

âœ… Repartition vs Coalesce  
âœ… Caching and persist()  
âœ… Broadcast join vs Shuffle join  
âœ… Catalyst Optimizer exploration  
âœ… Performance profiling

## Project Structure

- **notebooks/** : Jupyter/Colab notebook for interactive analysis  
- **src/** : Python scripts for reproducible runs  
- **data/** : (optional) store raw/simulated data if needed  
- **requirements.txt** : required packages

## How to Run

1. Clone the repository  
2. Set up a PySpark environment (e.g., Databricks or Colab with Java installed)  
3. Open `notebooks/PySpark_Ecommerce_Optimization.ipynb` and run cell by cell  

Or run `src/main.py` for a script-based flow.

## Technologies

- Python
- PySpark
- Catalyst Optimizer
- Broadcast Hash Join
- Partitioning techniques

## Author

- **Your Name**
- Data Science Intern at Evamp & Saanga

## License

This project is open-sourced under the MIT license.

